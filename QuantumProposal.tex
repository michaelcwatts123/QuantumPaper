\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{physics}
\usepackage{float}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{A Critical Comparison of Quantum versus Classic Data Science Algorithms\\


}

\author{\IEEEauthorblockN{Michael Watts}
\IEEEauthorblockA{\textit{Computer Science Department} \\
\textit{Southern Methodist University}\\
Dallas, United States of America \\
mcwatts@smu.edu}


}

\maketitle

\begin{abstract}
This paper will attempt to evaluate several quantum implementations of Classic Data Science algorithms in an effort to determine the maturity of the field of Quantum Data Science. We hope to show the viability of Quantum Data Science from a Classic Data Science perspective. To do this we will not only examine the algorithmic complexities of Quantum Data Science algorithms but also their possible weakness and implementation constraints. We will go through a step by step methodology examination of the mathematics behind each of these quantum implementations of algorithms. The goal of this work is to identify future research in the field to focus on to strength prepare Quantum Data Science for mainstream adoption in the future. 
\end{abstract}

\begin{IEEEkeywords}
Quantum Computing, Data Science, Unsupervised Learning, Supervised Learning, Data Mining, Artificial Intelligence, Quantum Data Science 
\end{IEEEkeywords}

\section{Introduction}
As the field of Quantum Computing expands, new and sophisticated quantum operations are invented. With these new operations, we can not only formulate novel quantum algorithms but also mirror classic algorithms in quantum systems. One emerging field of this algorithmic study is Quantum Data Science, which attempts to create quantum versions of classic Data Science algorithms. These quantum parallels can often take advantage of the unique properties of their data and operators in order to increase their computational efficiency. Several fundamental Data Science algorithms already have quantum parallels, including but not limited to: K-Means Clustering, Support Vector Machines, Principal Component Analysis, and Decision Trees \cite{b1} \cite{b2} \cite{b3} \cite{b4}. 
\newline
\indent In this paper, we will be examining the strengths and weakness of various quantum analogs to classic Data Science algorithms. Surveys and comparisons of quantum and classic Data Science algorithms have been preformed numerous times in the past. Schuld, Sinayskiy, and Petruccione in 2014 survyed various quantum versions of classic Data Science algorithms such as quantum Markov models and Quantum Bayesian classifiers. However, in this work they only do high level examinations of the mathematics behind these various methods, without going into explicit detail of the mathematics derivations powering these models or exact possible implementations of the models on quantum hardware \cite{b5}.
\newline
\indent Kopczyk continues this survey work. In his paper, he also demonstrates the exact mathematics powering several popular quantum subroutines. He then proceeds to show the mathematics for Quantum Data Science algorithms, building off of these various subroutines. Kopczyk also beings to examine the algorithmic complexity of these these quantum algorithms compared to their classic equivalents. He informs the reader of possible real world implementation concerns of the analyzed algorithms \cite{b6}. 
\newline
\indent Biamonte et al. furthers this focus on possible implementation in their survey work. They provide high level surveys of various Quantum Data Science algorithm implementations such as: Support Vector Machines and Boltzmann Machines. They also begin to examine how data can be read into quantum computers, different kinds of quantum processors, and the design and control of different quantum systems.\cite{b7}
\newline
\indent Finally Shrivastava, Soni, and Rasool gives high level algorithmic block diagrams of various Quantum Computing methods. They also provide more formal exact deviations for the algorithmic complexity of each examined quantum algorithm. There most important extension of the work is a focus on not only the Quantum Data Science algorithms but also on possible applications of the current implementations. This helps bound the author in a much more reality based examination of the algorithms.\cite{b8}    
\newline
\indent In this paper, we plan to continue expounding on prior survey research. We intended to do this by not only comparing the algorithmic complexity of quantum and classic Data Science algorithms but also by examining the complexities relative to the limitations of the respective algorithms. This will involve examining the various properties of each algorithm with respect to the improvements provided by its quantum implementation. We will also evaluate the practicality of implementation of these quantum algorithms. All of this is in effort to conclude if the field of Quantum Data Science is mature enough to begin the transition into mainstream usage. 
\newline
\indent This paper will ultimately serve as a benchmark on the the ability of Quantum Data Science to implement and improve basic algorithms of Classic Data Science. The rest of the paper is as follows: In Section II, we introduce important background high level information on Quantum Computing as well as each of the Classic Data Science algorithms examined in this paper. In Section 3, we derive quantum equivalents to the classic algorithms introduced in Section 2. In Section 4, we evaluate classic and quantum algorithms against one another as well as quantum algorithm improvements against the properties of the algorithms. In Section 5, we draw our final conclusions about the current state of Quantum Data Science.    
\section{Background Information}

\subsection{Quantum Computing}
In order to properly understand these algorithms, first we must understand the basics of Quantum Computing. Quantum algorithms tend to provide large computational speed ups to computers due to the nature of their data representation. Due to quantum particles being held in superposition between multiple possible states, we can execute operations on multiple states at the same time \cite{b5}. In a classic setting, you have a Turing machine in a single state, which when feed an instruction, evolves into another single state. In a quantum setting, you have a Turing machine existing in multiple states, that when feed an operation, can enter into multiple new states in parallel \cite{b9}.   
\subsubsection{Quantum Representation of Data}
We have already touched on how quantum bits or qubits represent data in a quantum system. We will formalize this representation here. A qubit can be thought of as a single classic bit that exists in a hybrid state between a 0 and a 1, known as a \emph{superposition} between state 0 and state 1. We represent that state in the following way:
\begin{align*}
\ket{\Psi} = \alpha\ket{0} + \beta\ket{1}
\end{align*}
In this form, $\alpha$ and $\beta$ are complex real numbers referred to as \emph{probability amplitudes}. According to a postulate of Quantum Mechanics know as Born's Rule, the probability that a qubit will collapse from its superposition into state 0 or state 1 is $|\alpha|^2$ and $|\beta|^2$ respectively. (Note measurement of quantum data will be described in a later section). In this form, the classic bit states 0 and 1 are described in \emph{Dirac Notation} as $\ket{0}$ and $\ket{1}$ where:
\begin{align*}
\ket{0}=\begin{vmatrix}
1\\
0\\
\end{vmatrix}
\ket{1}=\begin{vmatrix}
0\\
1\\
\end{vmatrix}
\end{align*}
\indent However, qubits can also be represented in a \emph{Density Matrix} often referred to as $\rho$. This matrix represents a combination of various pure quantum states and is used when we are missing information on the state of the quantum system. For our sake, a pure quantum state is simply one where know the exact state of the qubit. We can create a density matrix for a quantum system to represent its information using all of its possible quantum states and the probability \emph{p} it is in each respective state \emph{$\ket{\psi}$} by the following equation: 
\begin{align*}
\rho = \sum_{i}  = p_i \ket{\psi_i} \otimes \bra{\psi_i}
\end{align*}
This combination of various pure states is referred to a \emph{mixed state}.\cite{b9}
\subsubsection{Quantum Operations as Quantum Gates}
The key piece of information to understand about Quantum Operations is, they operate on the qubit's superposition and maintain the superposition. For example if we have the qubit $\psi$ described as:
\begin{align*}
\ket{\Psi} = \alpha\ket{0} + \beta\ket{1}
\end{align*}
which we can rewrite in vector notation as:

\begin{align*}
\ket{\Psi} = \alpha\ket{0} + \beta\ket{1} \\
 = \alpha \begin{vmatrix}
1\\
0\\
\end{vmatrix} + \beta \begin{vmatrix}
0\\
1\\
\end{vmatrix} \\
=  \begin{vmatrix}
\alpha\\
0\\
\end{vmatrix} +  \begin{vmatrix}
0\\
\beta\\
\end{vmatrix} \\
 =  \begin{vmatrix}
\alpha\\
\beta\\
\end{vmatrix} 
\end{align*}
We can then proceed to apply the quantum not operator upon this qubit. 
\begin{align*}
    \emph{X} = \begin{vmatrix}
    0 & 1 \\
    1 & 0 \\
    \end{vmatrix}
\end{align*}
This operator flips the probability amplitudes of the qubit without collapsing its superposition.
\begin{align*}
    \emph{X} * \begin{vmatrix}
\alpha\\
\beta\\
\end{vmatrix} = \begin{vmatrix}
\beta\\
\alpha\\
\end{vmatrix} 
\end{align*}
Any matrix may be a valid quantum operator so long as it is unitary: $U^\dagger U = I$. This can be see with the quantum not operator: 
\begin{align*}
    \begin{vmatrix}
    0 & 1 \\
    1 & 0 \\
    \end{vmatrix} * \begin{vmatrix}
    0 & 1 \\
    1 & 0 \\
    \end{vmatrix} =
    \begin{vmatrix}
    1 & 0 \\
    0 & 1 \\
    \end{vmatrix}
\end{align*}
It is also worth noting that every quantum operator is fully reversible \cite{b9}.
\subsubsection{Measurement of Quantum Data}
The measurement of a qubit is a destructive operation. Since as we have established, the qubit exists in a superposition of two states, when it is observed, it will collapse into one of its two possible states with probability $|\alpha|^2$ or $|\beta|^2$. Once this happens the superposition of the qubit is lost and it exists only in one of its classic states. This operation is only reversible if the measurement reveals no information about the qubit \cite{b9}. 

\subsection{Classic Data Science Algorithms}
Next, we will review the data science algorithms to better understand the equations we are attempting to mirror with our quantum logic. The following algorithms fall into two categories: Supervised and Unsupervised. Supervised Algorithms are those which require labeled data for training. This means that A data scientist would feed the algorithm both and X and Y data set. After this, he/she would give the algorithm new X values, and the algorithm would attempt to generate corresponding new Y values for the data set. Unsupervised algorithms can simply be fed X data and will attempt to transform or relate the data in some meaningful way. 
\subsubsection{K-Means Clustering}
The goal of K-Means clustering is to find \emph{K} clusters of data in a pool of unlabeled data. This makes it an unsupervised clustering algorithm. Assuming we had a pool of two dimensional unlabeled data, to preform a K-Means clustering, we would: First, create K random points of data, called centroids. Next for every vector of the data we would calculate its euclidean distance from each centroid vector with the following formula:
\begin{align*}
d_{euclidean} = \sqrt{(\overline{x}_{data} - \overline{x}_{centroid})^2}
\end{align*}
Then, we reassign each centroid to be the mean of all the data points assigned to it with the following formula:
\begin{align*}
\overline{x}_{centroid} + 1 = \frac{1}{\overline{x}_{centroid}}  \sum_{\bar{x}\in\overline{x}_{centroid}} \bar{x}
\end{align*}
We will repeat this process either a number of steps \emph{n} set by the user or until the difference between the current centroid and the prior centroid is below a certain tolerance threshold $\epsilon$ also set by the user\cite{b10}. 
\subsubsection{Principal Component Analysis}
Principal Component Analysis (PCA) is an unsupervised means of dimensionality reduction. To compute the PCA of a set of data of vectors $\bar{z}$ we must first compute the covariance matrix of $\bar{z}$ with the following equation:
\begin{align*}
Covariance = \sum_i \bar{z}_i\bar{z}^T_i
\end{align*}
\indent Next we will diagonalize the covariance by decomposing it into its eigenvectors \emph{$\bar{c}$} and its eigenvalues \emph{{y}}.
\begin{align*}
Covariance = \sum_i y_i\bar{c}_i\bar{c}_i^\dagger
\end{align*}
\indent We can then take a set of eigenvectors \emph{$\bar{c}$} with the largest eigenvalues \emph{{y}} and consider these to be the principal components. Finally, we simply project each $\bar{z}$ onto the principal components: 
\begin{align*}
    \bar{z} = \sum_i z_i \bar{c_i}
\end{align*}
Now our dimensionality is reduced down to our number of principal components \cite{b7}. 
\subsubsection{Decision Trees}
Decision Trees are a supervised learning method used for both classification and regression problems. To create a Decision Tree, we first start with a set of tree of node size 1 containing all data. For all features \emph{F} in our node, we will calculate the entropy \emph{H} of each class \emph{c} in \emph{C} in each feature. Entropy is simply a measure of the disorder of the data and can be calculated as:
\begin{align*}
    H(F) = \sum_{i\in C} -p_i log(p_i)
\end{align*}
Where \emph{$p_i$} is the probability of each class \emph{c} $\in$ \emph{C}. Our goal is to minimize \emph{H}. So we will pick the Feature with the minimal \emph{H} and split our node into \emph{c} children nodes along each class \emph{C} of the feature with the smallest entropy. We will repeat this algorithm until the entropy of each feature is below a tolerance $\epsilon$ or the algorithm has run for a set number of sets \emph{n} defined by the user \cite{b11}. 

\section{Derivation of Quantum Data Science Algorithms}
In this section, I will begin my working through the mathematics behind each of my chosen Quantum Data Science Algorithms. However, before I begin my algorithmic breakdown, I will first examine the inherit benefits of loading classic data in as qubits rather than bits. I feel this is an important strength of quantum systems and doing my survey without mentioning it would leave it incomplete. I plan to go over the circuit in \cite{b12} to show that N bits can be loaded into a quantum system as only $log_2(N)$ qubits.   
\subsection{Quantum K-Means}
Before we begin our breakdown of Quantum K-Means, we must go over two important quantum gates. The first is the Hadamard Gate. It is represented by the matrix:
\begin{align*}
\frac{1}{\sqrt{2}}
    \begin{vmatrix}
    1 & 1 \\
    1 & -1 \\
    \end{vmatrix} 
\end{align*} 
This gate takes a qubit in a basis state and places it in superposition between the basis states with a probability amplitude of $\frac{1}{\sqrt{2}}$ for each basis state. The next gate we will need is the swap gate. It is represented by the matrix: \begin{align*}
    \begin{vmatrix}
    1 & 0 & 0 & 0 \\
    0 & 0 & 1 & 0 \\
    0 & 1 & 0 & 0 \\
    0 & 0 & 0 & 1\\
    \end{vmatrix} 
\end{align*} 
This gate is applied to qubits in superposition between $\ket{0}$ and $\ket{1}$. It can be used to swap two qubits in the $\ket{1}$ position. For example, if your qubit in the $\ket{1}$ position was $\ket{1,x,y}$ after a swap gate, the qubit in $\ket{1}$ position would be $\ket{1,y,x}$ \cite{b5}
\newline
\indent With these gates, we can create what is known as a "Swap Test". This test allows us to find $\bra{x}\ket{y}$. \newline
\newline
{\large Swap Test} \newline
\noindent\rule{\columnwidth}{1pt}
\newline
   Initialize a qubit with a control basis qubit: \newline 
 \begin{align*}
 \ket{\theta_0} = \ket{0,x,y} 
 \end{align*} 
   Apply a Hadamard Gate to split this qubit into superposition: \newline
 \begin{align*}
 \ket{\theta_1} = \frac{1}{\sqrt{2}}(\ket{0,x,y}+ \ket{1,x,y}) 
 \end{align*} 
   Apply a Swap Gate: \newline 
 \begin{align*} 
 \ket{\theta_2} = \frac{1}{\sqrt{2}}(\ket{0,x,y}+ \ket{1,y,x}) 
 \end{align*} 
   Then another Hadamard Gate: \newline 
 \begin{align*}
 \ket{\theta_3} = \frac{1}{2}(\ket{0}(\ket{a,b}+\ket{b,a}) + \ket{1}(\ket{a,b}-\ket{b,a}) 
 \end{align*} 
 Now we simply measure our control qubit \newline
 \begin{align*}
 P(\ket{0}) =|.5 \bra{0}\ket{0}(\ket{x,y} + \ket{y,x}) + .5\bra{0}\ket{1}(\ket{x,y}-\ket{y,x})|^2
 \end{align*}
 Since $\ket{0}$ and $\ket{1}$ are orthogonal the second term disappears and our result simplifies to: \newline
 \begin{align*}
 P(\ket{0}) = .5 + .5|\bra{a}\ket{b}|^2
 \end{align*} \newline
\noindent\rule{\columnwidth}{1pt}
\indent We know if $P(\ket{0})$ = .5 for our control qubit, our qubits $x$ and $y$ are orthogonal. We do however require identical sets of $x$ and $y$ qubits such that we can repeat this measurement to accurately determine the probability. We can use $\bra{x}\ket{y}$ to measure distance between the x and y if they are prepared in the proper initial superposition. This is done with a method called "DistCalc".  \newline
\newline
{\large DistCalc} \newline
\noindent\rule{\columnwidth}{1pt}
Assume we have two qubits in super position:
\begin{align*}
& \ket{\lambda} = \frac{1}{\sqrt{2}}(\ket{0,x} + \ket{1,y}) \\
& \ket{\sigma} = \frac{1}{\sqrt{D}}(|x|\ket{0} + |y|\ket{1})
\end{align*} 
Where \emph{D} is: 
\begin{align*}
D = |x|^2+|y|^2. 
\end{align*}  
We know that:  
\begin{align*}
 \bra{\sigma}\ket{\lambda} = \frac{1}{\sqrt{2D}}(|x|\ket{x} - |b|\ket{b})
\end{align*} 
Which can be rearranged to: 
\begin{align*}
|x-y|^2 = 2D|\bra{\sigma}\ket{\lambda}|^2
\end{align*} 
Where $|x-y|^2$ is the distance between x and y. We can calculate $\bra{\sigma}\ket{\lambda}$ with our Swap Test. 
\newline
\noindent\rule{\columnwidth}{1pt}
\newline
\indent Using this "DistCalc" method, we are able to perform our K-Means Clustering. \newline
\newline
{\large K Means Clustering} \newline
\noindent\rule{\columnwidth}{1pt}
Loop \newline
\indent For each point \emph{p} in \emph{P} \newline
\indent \indent Calculate distance from \emph{p} to each centroid with \newline 
\indent \indent DistCalc(p, centroid) \newline
\indent \indent Assign \emph{p} to cluster of closest centroid \newline
\indent For each \emph{c} in Clusters \emph{C} \newline
\indent \indent For each point \emph{p} in \emph{c} \newline
\indent \indent \indent Calculate mean value of all points \emph{P} \newline
\indent \indent \indent Set value of \emph{c} centroid to mean \newline
\indent If Stopping Criteria is met \newline
\indent \indent Stop \newline
\indent Else \newline
\indent\indent Loop \newline
\noindent\rule{\columnwidth}{1pt}
\cite{b6}.
\subsection{Quantum Principal Component Analysis}
Before we can begin our in depth analysis of Quantum PCA, we must sent the proper initial conditions. We will assume we have a density matrix $\rho$ composed over several qubits, each representing a vector in our data. By definition, $\rho$ will be a hermitian matrix. However, in order to calculate its eigenvalues, we will need to convert our density matrix into a unitary matrix. To do this, we will need two mathematical laws, Matrix Exponentiation and Euler's Identity. First Matrix Exponentiation states:    
\begin{align*}
U = e^{iXY}
\end{align*} 
Where U is a unitary matrix, X is a hermitian matrix, and Y is a scalar value. Secondly, we will employ Euler's Identity: 
\begin{align*}
e^{iXY} = Cos(Y)I + iSin(Y)X
\end{align*} 
Where X is a matrix, Y is a scalar, and I is the identity matrix \cite{b16}. Using these two laws in tandem, we can convert $\rho$ to a unitary matrix by doing the following:
\begin{align*}
 U = e^{i\rho Y} = Cos(Y)I + iSin(Y)\rho
\end{align*} 
We must do this as the method we are going to introduce requires a unitary matrix as input. We must also introduce two more quantum gates, The Controlled Unitary Gate and the Inverse Quantum Fourier Transform Gate. The Controlled Unitary Gate is represented by the unitary matrix: 
 \begin{align*}
    \begin{vmatrix}
    1 & 0 \\
    0 & 1  \\
    \end{vmatrix}
 \end{align*}  
This gate is controlled, which means the operation only happens on the qubit if another qubit (called a controlled qubit) is in a certain state. For our work, this operation will only happen if we have a control qubit in $\ket{1}$.  
\newline
\indent The next gate is the Inverse Quantum Fourier Transform Gate. The nuances of this gates are beyond the scope of our paper. For this paper, it is sufficient merely to think of this gate as an operation which changes the qubit into a different basis state. It is represented by the matrix: 
\begin{align*}
\frac{1}{\sqrt{2}}
    \begin{vmatrix}
    1 & 1 \\
    1 & e^{-\pi i} \\
    \end{vmatrix} 
\end{align*} 
\indent With these two gates, we are able to preform a Quantum Phase Estimation:\newline
\newline
{\large Quantum Phase Estimation} \newline
\noindent\rule{\columnwidth}{1pt}
Assume we have two qubits, a control qubit $\ket{C}$ initialized to
\begin{align*}
\ket{C} = \ket{0}
\end{align*} 
and a value qubit $\ket{X}$ initialized as some arbitrary basis. So we have: 
\begin{align*}
\ket{C} \ket{X}
\end{align*} 
\newline 
Apply a Hadamard Gate to split $\ket{C}$ into superposition to give us:
\begin{align*}
    \ket{C} = \frac{1}{\sqrt{2}}(\ket{0}+ \ket{1}) 
\end{align*}
Apply a Controlled Unitary Gate to $\ket{C}$ to transform it into a Fourier State. 
\begin{align*}
    \ket{C} = \frac{1}{\sqrt{2}}(\ket{0}+ e^{2 \pi i \alpha}\ket{1}) 
\end{align*}
Apply an Inverse Quantum Fourier Transform Gate to $\ket{C}$ to change the basis state of the control qubit.
\begin{align*}
    \ket{C} = \frac{1}{2}     \begin{vmatrix}
    1 + e^{2 \pi i \alpha} \\
    1 + e^{\pi i} \\
    \end{vmatrix} 
\end{align*}
Which is an equivalent expression to: 
\begin{align*}
    \ket{C} = .5 \sum_{a = 0}^1 \sum_{b = 0}^1 e^{2\pi i k(\alpha - j)}\ket{a}
\end{align*}
Our end results is: 
\begin{align*}
\ket{C} \ket{X} = .5 \sum_{a = 0}^1 \sum_{b = 0}^1 e^{2\pi i k(\alpha - j)}\ket{a} \ket{X}
\end{align*} 
If we measure the state $\ket{2 \alpha}$. It's probability of collapsing into this state is: 
\begin{align*}
P(\ket{2 \alpha}) = |.5 \bra{2 \alpha}\sum_{a = 0}^1 \sum_{b = 0}^1 e^{2\pi i k(\alpha - j)}\ket{a} |^2
\end{align*} 
Which simplifies down to:
\begin{align*}
P(\ket{2 \alpha}) = |.5  \sum_{b = 0}^1  \bra{2 \alpha}\ket{2 \alpha}|^2
\end{align*} 
Since we know $\bra{2 \alpha}$ and $\ket{2 \alpha}$ are orthonormal, we know their overlap will simplify down to 1. Thus our expression simplifies down further: 
\begin{align*}
& P(\ket{2 \alpha}) = |.5  \sum_{b = 0}^1  1|^2 \\
& P(\ket{2 \alpha}) = |.5  (1+1)|^2 \\
& P(\ket{2 \alpha}) = 1 \\
\end{align*} 
\noindent\rule{\columnwidth}{1pt}
\indent If we apply the Quantum Phase Estimation to a density matrix $\rho$, we know that it will collapse into eigenvectors with a probability of that eigenvector's respective eigenvalue \cite{b6}. \newline
\indent With this, we are finally able to create our Quantum Principal Component Analysis. \newline \newline
{\large Quantum Principal Component Analysis} \newline
\noindent\rule{\columnwidth}{1pt}
Create Density Matrix $\rho$ from all available qubits. \newline
Loop \newline
\indent Create \emph{N} copies of $\rho$ \newline
\indent For each \emph{n} in \emph{N} \newline 
\indent \indent Perform Phase Estimation of \emph{n} \newline 
\indent If  all eigenvectors \emph{V} and eigenvalues $\Lambda$ are found \newline 
\indent \indent Stop \newline 
\indent Else \newline 
\indent\indent Loop \newline 
Sort \emph{V} by $\Lambda$ \newline 
Return \emph{V} and $\Lambda$ to User \newline 
\noindent\rule{\columnwidth}{1pt}
\cite{b7}
\subsection{Quantum Decision Trees}
As prior described, in a Classic Decision Tree, we identify the feature along which to split our current node into new nodes, by that feature's entropy. Classic Entropy has a quantum equivalent known as Von Neumann Entropy. Given a density matrix $\rho$ of \emph{N} qubits, the Von Neumann Entropy can be calculated as:
\begin{align*}
    S(\rho) = -\sum_i \lambda_i log(\lambda_i)
\end{align*}
Where $\lambda$ is an eigenvalue of $\rho$ for all eigenvalues of $\rho$ \cite{b9}. With this, we have all we need to create our Quantum Decision Tree. \newline \newline
{\large Quantum Decision Tree} \newline
\noindent\rule{\columnwidth}{1pt}
Loop \newline
\indent For each \emph{f} in Features \emph{F} \newline
\indent \indent For each Class \emph{c} in \emph{f} \newline
\indent \indent \indent Create $\rho$ from all instances of \emph{c} in node \newline
\indent \indent \indent Use Quantum Principal Component Analysis to get \indent \indent \indent all eigenvalues of $\rho$ \newline
\indent \indent Sum Von Neumann Entropy of all \emph{c} in \emph{f} \newline
\indent Identify \emph{f} with minimum Von Neumann Entropy \newline
\indent \indent For each \emph{c} in minimum \emph{f} \newline
\indent \indent \indent Create new node \newline
\indent \indent \indent Add all instances of \emph{c} from old node into new node \newline
\indent If Stopping Criteria is met \newline
\indent \indent Stop \newline
\indent Else \newline
\indent \indent Loop \newline
\section{Results}
We will now compare the relative merit of each quantum algorithm explored against not just its classic equivalent but also against the constraints of the algorithms themselves.
\subsection{Space Efficiency}
However, before we can begin our examination of each algorithm, we must address one glaring point of superiority all quantum algorithms will have over classic. Since a qubit is able to represent not just individual states but superpositions of states, each qubit is able to hold more information than a classic bit. This means we can represent \emph{N} bits as \emph{M} qubits, where \emph{M} $ = Log_2(N)$ \cite{b12}.
\subsection{K-Means Clustering}
We will begin our comparison by first examining the algorithmic complexity of each implementation. 
\begin{table}[H]
\centering
\caption{Algorithmic Comparisons}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
 Implementation & Run Time   \\
\hline
Quantum & O(\emph{MK}log(\emph{N})) \\
\hline
Classic & O(\emph{MKN}) \\
\hline
\end{tabular}
\end{table}
Where \emph{M} is the number of data points, \emph{N} is the dimensionality of the data points, and \emph{K} is the number of clusters \cite{b6}. This means that as \emph{N} increases, the quantum implementation of the algorithm's performances becomes better and better relative to the classic algorithm. However, as \emph{N} increases, K-Means Clustering becomes less and less of a viable choice. This is due to the fact that as the dimensionality of the data increases, the distance between any one data to point to all others begins to converge. Since all data is equally spaced out, it becomes more and more difficult to form meaningful clusters of the data \cite{b15}. The superiority of the quantum implementation becomes more and more meaningful as \emph{N} increases. However, it is also undermined by the fact K-Means Clustering is becomes more and more nonfunctional as \emph{N} increases.  
\subsection{Principal Component Analysis}
One major constraint of our Quantum Principal Component Analysis not present in the classic implementation is its dependence on there being a low number of principal components relative to the dimensionality of the data \cite{b6}. As this is a method of dimensionality reduction, one would like this to be the case for our data. However, a data scientist is not aware of the number of principal components needed to represent his/her data and could therefore run into this edge case very easily. This constraint undermines the usability of the algorithm as it requires prior knowledge a user cannot be expected to posses. \newline
\indent Another constraint of this quantum algorithm not present in the classic algorithm is the need to make multiple copies of qubits in order to determine the eigenvalues and eigenvectors of the data. This is essentially a brute force method of determining these values. This leads to inflated numbers of qubits needed to perform this algorithm.  
\subsection{Decision Trees}
The only found constraints on Quantum Decision Trees are those that come from the usage of Quantum Principal Component Analysis inside of the algorithm. Otherwise, it improves on the classic version by not requiring knowledge of the probabilities of classes in order to determine entropy. Also by creating density matrices of all instances of a class, we are able to leverage information parallelism in order to rapidly calculate the Von Neumann Entropy of classes.   

\section{Conclusion}
We wished to write this paper in order to examine Quantum Data Science in a more grounded state. We wanted to see not what the field could be, but what it currently is, in an effort to properly evaluate it. In this paper we will have examined the mathematics behind several quantum implementation of classic Data Science algorithms. We have found several possible shortcomings of current implementations of quantum algorithms that need addressing before they are ready for mainstream adoption. In particular, research needs to be dedicated to finding new means of extracting values from qubits rather than simply making many copies of the qubits and measuring all of them. Also the algorithmic speedups of quantum algorithms need to be in manners that do not emphasize the weaknesses of algorithms.   

\begin{thebibliography}{00}

\bibitem{b1}Jing Xiao, YuPing Yan, Jun Zhang, Yong Tang.”A quantum-inspired genetic algorithm for k-means clustering”. Elsevier
journal (37), 4966-4973. 

\bibitem{b2}P. Rebentrost, M. Mohseni, and S. Lloyd, “Quantum support vector machine for big data classification,” Phys. Rev. Lett., vol. 113, no. 13, p. 130503, Sep. 2014, doi: 10.1103/PhysRevLett.113.130503.

\bibitem{b3} S. Lloyd, M. Mohseni, and P. Rebentrost, Quantum principal component analysis, arXiv preprint 1307.0401 (2013).

\bibitem{b4}S. Lu and S. L. Braunstein, “Quantum decision tree classifier,” Quantum Inf Process, vol. 13, no. 3, pp. 757–770, Mar. 2014, doi: 10.1007/s11128-013-0687-5.

\bibitem{b5}M. Schuld, I. Sinayskiy, and F. Petruccione, “An introduction to quantum machine learning,” Contemporary Physics, vol. 56, no. 2, pp. 172–185, Apr. 2015, doi: 10.1080/00107514.2014.964942.

\bibitem{b6}D. Kopczyk, “Quantum machine learning for data scientists,” arXiv:1804.10068 [quant-ph], Apr. 2018, Accessed: Apr. 25, 2020. [Online]. Available: http://arxiv.org/abs/1804.10068.

\bibitem{b7}J. Biamonte, P. Wittek, N. Pancotti, P. Rebentrost, N. Wiebe, and S. Lloyd, “Quantum Machine Learning,” Nature, vol. 549, no. 7671, pp. 195–202, Sep. 2017, doi: 10.1038/nature23474.

\bibitem{b8}P. Shrivastava, K. K. Soni, and A. Rasool, “Classical Equivalent Quantum Unsupervised Learning Algorithms,” Procedia Computer Science, vol. 167, pp. 1849–1860, Jan. 2020, doi: 10.1016/j.procs.2020.03.204.

\bibitem{b9} M. A. Nielsen and I. L. Chuang, Quantum computation and quantum information. Cambridge: Cambridge University Press, 2019.

\bibitem{b10} C. D. Manning, P. Raghavan, and H. Schütze, Introduction to information retrieval. Cambridge: Cambridge University Press, 2018.

\bibitem{b11} A. Burkov, The hundred-page machine learning book. S.l.: Andriy Burkov, 2019.

\bibitem{b12} J. A. Cortese and T. M. Braje, “Loading Classical Data into a Quantum Computer,” arXiv:1803.01958 [quant-ph], Mar. 2018, Accessed: Apr. 26, 2020. [Online]. Available: http://arxiv.org/abs/1803.01958.

\bibitem{b13} K. Khadiev, I. Mannapov, and L. Safina, “The Quantum Version Of Classification Decision Tree Constructing Algorithm C5.0,” arXiv:1907.06840 [quant-ph, stat], Jul. 2019, Accessed: Apr. 26, 2020. [Online]. Available: http://arxiv.org/abs/1907.06840.

\bibitem{b14} X. Sun and Y. Zheng, “Hybrid Decision Trees: Longer Quantum Time is Strictly More Powerful,” arXiv:1911.13091 [quant-ph], Nov. 2019, Accessed: Apr. 26, 2020. [Online]. Available: http://arxiv.org/abs/1911.13091.

\bibitem{b15} “k-Means Advantages and Disadvantages | Clustering in Machine Learning,” Google Developers. https://developers.google.com/machine-learning/clustering/algorithm/advantages-disadvantages (accessed Apr. 26, 2020).

\bibitem{b16}E. W. Weisstein, “Euler Formula.” https://mathworld.wolfram.com/EulerFormula.html (accessed May 08, 2020).




\end{thebibliography}


\end{document}
